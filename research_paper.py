# -*- coding: utf-8 -*-
"""Research paper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fQTfxsHBZkWSKyQzENRjX4fgTaT1_mRc
"""

import os
import pandas as pd
import matplotlib.pyplot as plt
import fitz  # PyMuPDF
from transformers import GPT2TokenizerFast
from sentence_transformers import SentenceTransformer
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
import requests

# Store your Gemini API key securely in the environment variable (for security purposes)
os.environ["GEMINI_API_KEY"] = "AIzaSyD1eHH9Qq1JmrNZlhr-fRJtDvuePpUXFuY"  # Replace with your Gemini API key

# Retrieve your Gemini API key
api_key = os.getenv("GEMINI_API_KEY")

# Function to load PDF using PyMuPDF
def load_pdf(file_path):
    doc = fitz.open(file_path)
    text = ""
    for page_num in range(doc.page_count):
        page = doc[page_num]
        text += page.get_text()
    return text

# Load your PDF
pdf_text = load_pdf("/content/1.pdf")  # Replace with the path to your PDF

# Initialize tokenizer (not needed for token counting, but kept for any further token-related operations)
tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")

# Split text into chunks (token counting removed)
from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=24,
)
chunks = text_splitter.create_documents([pdf_text])

# Generate embeddings using sentence-transformers
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')

# Create FAISS vector store
db = FAISS.from_documents(chunks, embeddings)

# Define Gemini API integration
def query_gemini(text, api_key):
    url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent"
    headers = {"Content-Type": "application/json"}
    data = {
        "contents": [{"parts": [{"text": text}]}]
    }
    response = requests.post(url, headers=headers, json=data, params={"key": api_key})

    print(f"API Response Status: {response.status_code}")
    if response.status_code == 200:
        result = response.json()
        print(f"API Response Content: {result}")  # Print the raw response for debugging
        # Extract the correct text from the API response
        if 'candidates' in result and len(result['candidates']) > 0:
            generated_text = result['candidates'][0]['content']['parts'][0]['text']
            return generated_text
        else:
            return "No valid response from Gemini."
    else:
        print(f"Error: {response.status_code}, {response.text}")
        return None

# Function to ask Gemini with PDF context
def ask_gemini_with_pdf(query):
    # Retrieve similar documents from the PDF vector database
    docs = db.similarity_search(query)
    context = "\n\n".join([doc.page_content for doc in docs])

    # Combine context and query for a comprehensive answer
    full_query = f"Context: {context}\n\nQuestion: {query}"
    response = query_gemini(full_query, api_key)
    return response

# Example Usage:
query = "Give me the author names,and what are the technologies  and methods they used and how much accuracy they got.and merits. give me details heading wise?"
response = ask_gemini_with_pdf(query)
print(f"Question: {query}")
print(f"Answer: {response}")

#query = "Give me the author names,and what are the technologies  and methods they used and how much accuracy they got.and merits. give me details heading wise?"
query="how much accuracy they got?"
response = ask_gemini_with_pdf(query)
print(f"Question: {query}")
print(f"Answer: {response}")

!pip install PyMuPDF
!pip install -U langchain-community

!pip install faiss
!pip install faiss-cpu

!pip install streamlit pyngrok pymupdf langchain sentence-transformers transformers

# Commented out IPython magic to ensure Python compatibility.
# # Install necessary libraries
# #!pip install streamlit pyngrok pymupdf langchain sentence-transformers transformers
# 
# # Write the Streamlit app code to a file
# %%writefile pdf_query_app.py
# import os
# import streamlit as st
# import fitz  # PyMuPDF
# from sentence_transformers import SentenceTransformer
# from langchain.embeddings import HuggingFaceEmbeddings
# from langchain.vectorstores import FAISS
# import requests
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# 
# # Set your Gemini API key securely
# os.environ["GEMINI_API_KEY"] = "AIzaSyD1eHH9Qq1JmrNZlhr-fRJtDvuePpUXFuY"  # Replace with your Gemini API key
# api_key = os.getenv("GEMINI_API_KEY")
# 
# # PDF Upload and Loading Function
# def load_pdf(file):
#     doc = fitz.open(stream=file.read(), filetype="pdf")
#     text = ""
#     for page_num in range(doc.page_count):
#         page = doc[page_num]
#         text += page.get_text()
#     return text
# 
# # Define Gemini API Integration
# def query_gemini(text):
#     url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent"
#     headers = {"Content-Type": "application/json"}
#     data = {"contents": [{"parts": [{"text": text}]}]}
#     response = requests.post(url, headers=headers, json=data, params={"key": api_key})
# 
#     if response.status_code == 200:
#         result = response.json()
#         if 'candidates' in result and len(result['candidates']) > 0:
#             generated_text = result['candidates'][0]['content']['parts'][0]['text']
#             return generated_text
#         else:
#             return "No valid response from Gemini."
#     else:
#         return f"Error: {response.status_code}, {response.text}"
# 
# # Main Streamlit app function
# def main():
#     st.title("PDF Query App with Gemini AI")
#     st.write("Upload a PDF, ask a question, and get an AI-generated answer based on PDF content.")
# 
#     # PDF Upload in Streamlit
#     uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")
#     if uploaded_file is not None:
#         pdf_text = load_pdf(uploaded_file)
# 
#         # Initialize embeddings and vector store
#         text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=24)
#         chunks = text_splitter.create_documents([pdf_text])
# 
#         embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
#         embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')
#         db = FAISS.from_documents(chunks, embeddings)
# 
#         # User Query Input
#         query = st.text_input("Enter your question:")
#         if st.button("Submit Query"):
#             if query:
#                 docs = db.similarity_search(query)
#                 context = "\n\n".join([doc.page_content for doc in docs])
#                 full_query = f"Context: {context}\n\nQuestion: {query}"
#                 response = query_gemini(full_query)
# 
#                 # Display response
#                 st.subheader("Answer:")
#                 st.write(response)
# 
# if __name__ == "__main__":
#     main()
# 
# # Run Streamlit with IP address retrieval
# !wget -q -O - ipv4.icanhazip.com
# !streamlit run pdf_query_app.py & npx localtunnel --port 8501
#

# with out suggestions
# Step 1: Install required packages
!pip install streamlit pyngrok pymupdf langchain sentence-transformers transformers

# Step 2: Write the Streamlit code to a Python file
with open("pdf_query_app.py", "w") as f:
    f.write("""
import os
import streamlit as st
import fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
import requests
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Set your Gemini API key securely
os.environ["GEMINI_API_KEY"] = "AIzaSyD1eHH9Qq1JmrNZlhr-fRJtDvuePpUXFuY"  # Replace with your Gemini API key
api_key = os.getenv("GEMINI_API_KEY")

# PDF Upload and Loading Function
def load_pdf(file):
    doc = fitz.open(stream=file.read(), filetype="pdf")
    text = ""
    for page_num in range(doc.page_count):
        page = doc[page_num]
        text += page.get_text()
    return text

# Define Gemini API Integration
def query_gemini(text):
    url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent"
    headers = {"Content-Type": "application/json"}
    data = {"contents": [{"parts": [{"text": text}]}]}
    response = requests.post(url, headers=headers, json=data, params={"key": api_key})

    if response.status_code == 200:
        result = response.json()
        if 'candidates' in result and len(result['candidates']) > 0:
            generated_text = result['candidates'][0]['content']['parts'][0]['text']
            return generated_text
        else:
            return "No valid response from Gemini."
    else:
        return f"Error: {response.status_code}, {response.text}"

# Main Streamlit app function
def main():
    st.title("PDF Query App with Gemini AI")
    st.write("Upload a PDF, ask a question, and get an AI-generated answer based on PDF content.")

    # PDF Upload in Streamlit
    uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")
    if uploaded_file is not None:
        pdf_text = load_pdf(uploaded_file)

        # Initialize embeddings and vector store
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=24)
        chunks = text_splitter.create_documents([pdf_text])

        embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')
        db = FAISS.from_documents(chunks, embeddings)

        # User Query Input
        query = st.text_input("Enter your question:")
        if st.button("Submit Query"):
            if query:
                docs = db.similarity_search(query)
                context = "\\n\\n".join([doc.page_content for doc in docs])
                full_query = f"Context: {context}\\n\\nQuestion: {query}"
                response = query_gemini(full_query)

                # Display response
                st.subheader("Answer:")
                st.write(response)

if __name__ == "__main__":
    main()
    """)

# Step 3: Run Streamlit with IP address retrieval
!wget -q -O - ipv4.icanhazip.com
!streamlit run pdf_query_app.py & npx localtunnel --port 8501

#suggesions going to chat box
# Step 2: Write the Streamlit code to a Python file
with open("pdf_query_app.py", "w") as f:
    f.write("""
import os
import streamlit as st
import fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
import requests
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Set your Gemini API key securely
os.environ["GEMINI_API_KEY"] = "AIzaSyD1eHH9Qq1JmrNZlhr-fRJtDvuePpUXFuY"  # Replace with your Gemini API key
api_key = os.getenv("GEMINI_API_KEY")

# PDF Upload and Loading Function
def load_pdf(file):
    doc = fitz.open(stream=file.read(), filetype="pdf")
    text = ""
    for page_num in range(doc.page_count):
        page = doc[page_num]
        text += page.get_text()
    return text

# Define Gemini API Integration
def query_gemini(text):
    url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent"
    headers = {"Content-Type": "application/json"}
    data = {"contents": [{"parts": [{"text": text}]}]}
    response = requests.post(url, headers=headers, json=data, params={"key": api_key})

    if response.status_code == 200:
        result = response.json()
        if 'candidates' in result and len(result['candidates']) > 0:
            generated_text = result['candidates'][0]['content']['parts'][0]['text']
            return generated_text
        else:
            return "No valid response from Gemini."
    else:
        return f"Error: {response.status_code}, {response.text}"

# Main Streamlit app function
def main():
    st.title("PDF Query App with Gemini AI")
    st.write("Upload a PDF, ask a question, and get an AI-generated answer based on PDF content.")

    # Suggested questions
    suggestions = [
        "What are the key findings of this research paper?",
        "Can you summarize the methodology used?",
        "What is the main objective of the study?",
        "List the technologies and methods used.",
        "What are the limitations mentioned in the study?"
    ]

    # PDF Upload in Streamlit
    uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")
    if uploaded_file is not None:
        pdf_text = load_pdf(uploaded_file)

        # Initialize embeddings and vector store
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=24)
        chunks = text_splitter.create_documents([pdf_text])

        embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')
        db = FAISS.from_documents(chunks, embeddings)

        # User Query Input and Suggestions
        if "query" not in st.session_state:
            st.session_state.query = ""

        query = st.text_input("Enter your question:", value=st.session_state.query)

        st.write("Or select a question below:")
        for question in suggestions:
            if st.button(question):
                st.session_state.query = question  # Store the selected suggestion in session_state
                st.rerun()  # Rerun the app to update the query input with the selected question

        # Process query
        if st.button("Submit Query") and query:
            docs = db.similarity_search(query)
            context = "\\n\\n".join([doc.page_content for doc in docs])
            full_query = f"Context: {context}\\n\\nQuestion: {query}"
            response = query_gemini(full_query)

            # Display response
            st.subheader("Answer:")
            st.write(response)

if __name__ == "__main__":
    main()
    """)

# Step 3: Run Streamlit with IP address retrieval
!wget -q -O - ipv4.icanhazip.com
!streamlit run pdf_query_app.py & npx localtunnel --port 8501

#submit button below the chat box
with open("pdf_query_app.py", "w") as f:
    f.write("""
import os
import streamlit as st
import fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
import requests
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Set your Gemini API key securely
os.environ["GEMINI_API_KEY"] = "AIzaSyD1eHH9Qq1JmrNZlhr-fRJtDvuePpUXFuY"  # Replace with your Gemini API key
api_key = os.getenv("GEMINI_API_KEY")

# PDF Upload and Loading Function
def load_pdf(file):
    doc = fitz.open(stream=file.read(), filetype="pdf")
    text = ""
    for page_num in range(doc.page_count):
        page = doc[page_num]
        text += page.get_text()
    return text

# Define Gemini API Integration
def query_gemini(text):
    url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent"
    headers = {"Content-Type": "application/json"}
    data = {"contents": [{"parts": [{"text": text}]}]}
    response = requests.post(url, headers=headers, json=data, params={"key": api_key})

    if response.status_code == 200:
        result = response.json()
        if 'candidates' in result and len(result['candidates']) > 0:
            generated_text = result['candidates'][0]['content']['parts'][0]['text']
            return generated_text
        else:
            return "No valid response from Gemini."
    else:
        return f"Error: {response.status_code}, {response.text}"

# Main Streamlit app function
def main():
    st.title("Chat With Researach Paper")
    st.write("Upload a PDF, ask a question, and get an AI-generated answer based on PDF content.")

    # Suggested questions
    suggestions = [
        "What are the key findings of this research paper?","Can you summarize the methodology used?",
        "What is the main objective of the study?",
        "List the technologies and methods used.",
        "What are the limitations mentioned in the study?"
    ]

    # PDF Upload in Streamlit
    uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")
    if uploaded_file is not None:
        pdf_text = load_pdf(uploaded_file)

        # Initialize embeddings and vector store
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=24)
        chunks = text_splitter.create_documents([pdf_text])

        embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')
        db = FAISS.from_documents(chunks, embeddings)

        # User Query Input
        if "query" not in st.session_state:
            st.session_state.query = ""

        query = st.text_input("Enter your question:", value=st.session_state.query)

        # Separate the Submit button from the chat box
        if st.button("Submit Query") and query:
            docs = db.similarity_search(query)
            context = "\\n\\n".join([doc.page_content for doc in docs])
            full_query = f"Context: {context}\\n\\nQuestion: {query}"
            response = query_gemini(full_query)

            # Display response
            st.subheader("Answer:")
            st.write(response)

        # Add extra space for suggestions below the button
        st.write("")  # Blank line for spacing
        st.write("Or select a question below:")

        # Display suggestion buttons
        for question in suggestions:
            if st.button(question):
                st.session_state.query = question  # Store the selected suggestion in session_state
                st.rerun()  # Rerun the app to update the query input with the selected question

if __name__ == "__main__":
    main()
    """)


!wget -q -O - ipv4.icanhazip.com

!streamlit run pdf_query_app.py & npx localtunnel --port 8501

!wget -q -O - ipv4.icanhazip.com

!streamlit run pdf_query_app.py & npx localtunnel --port 8501

